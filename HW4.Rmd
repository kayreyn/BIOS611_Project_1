---
title: "HW4 - Weight and Height to Predict Gender"
output: pdf_document
---


```{r, results='hide', message=FALSE, echo = FALSE}
### Libraries
library(tidyverse)
library(gbm)
library(MLmetrics)
library(stats)
library(pROC)
```

### Data
```{r, results='hide', message=FALSE, echo = FALSE}

# Filepath
gen.file <- "./source_data/gen_height_weight.txt"

# Load
gen <- read_delim(gen.file, col_names = TRUE, delim = "\t")

# Examine
str(gen)

# Set Seed
set.seed(101120202)
```

The graph below illustrates the relationship and distribution of height and weight by gender.  There appears to be no clear difference between the two groups by gender, since this data was randomly generated and does not represent actual distributions. 

```{r, echo = FALSE, message = FALSE}
# Initial Graph
ggplot(gen, aes(Height, Weight, color = Gender)) +
	geom_point() +
	labs(color = "Gender") +
	scale_color_manual(labels = c("Male", "Female"), values = c("blue", "red")) +
	ggtitle("Gender from Height and Weight")
```


### 1 - Build a GLM.
```{r}
# Assign Group
gen <- gen %>% mutate(Set = sample(c("Train", "Validate", "Test"), size = nrow(gen), prob = c(.7, .15,.15), replace= TRUE))

# Gender Column becomes 0 or 1
gen <- gen %>% mutate(Gender = c(Gender == "Male"))

# Split up by Group
train <- gen %>% filter(Set == "Train")
validate <- gen %>% filter(Set == "Validate")
test <- gen %>% filter(Set == "Test")

# Build Model
model <- glm(Gender ~ Height + Weight, family = binomial(link = 'logit'), data = train)

# Predict Genders
pred <- predict(model, newdata = validate, type = "response")

# Accuracy
glm.accuracy <- round(sum((pred > 0.5) == validate$Gender) / nrow(validate), 3)
```
The accuracy of the GLM model is `r glm.accuracy`.  This is a very poor accuracy, but since there is no difference in gender height and weight distributions in this data the result is understandable. 



### 2 - Build a GBM

```{r, results='hide', message=FALSE}
# Build Model
gbm.model <- gbm(Gender ~ Height + Weight, distribution = "bernoulli",
			 data = train,
       n.trees = 100,
       interaction.depth = 2,
       shrinkage = 0.1)

# Predict Genders
gbm.pred <- predict(gbm.model, newdata = validate, type = "response")

# Accuracy
gbm.accuracy <- round(sum((gbm.pred > 0.5) == validate$Gender) / nrow(validate), 3)
```
The accuracy of the GBM model is `r gbm.accuracy`.  This accuracy is still not very good and is not significantly better than the GLM model.


### 3 - Only 50 Males

```{r, results='hide', message=FALSE}
# All F, 50 M
gen.male <- gen %>% filter(Gender == TRUE) %>% sample_n(size = 50, replace = FALSE)
gen.female <- gen %>% filter(Gender == FALSE)

# Combine
few.m <- full_join(gen.male, gen.female)

# New Set Assignments
few.m <- few.m %>% mutate(Set = sample(c("Train", "Validate", "Test"), size = nrow(few.m), prob = c(.7, .15,.15), replace= TRUE))

# Split Anew
train.m <- few.m %>% filter(Set == "Train")
validate.m <- few.m %>% filter(Set == "Validate")
test.m <- few.m %>% filter(Set == "Test")

# GBM Model, fewer Men
# Build Model
gbm.model.m <- gbm(Gender ~ Height + Weight, distribution = "bernoulli",
			 data = train.m)

# Predict Genders
gbm.pred.m <- predict(gbm.model.m, newdata = validate.m, type = "response")

# Accuracy
fifty.accuracy <- round(sum((gbm.pred.m > 0.5) == validate.m$Gender) / nrow(validate), 3)

# F1
# F1_Score(y_true = validate.m$Gender, y_pred = gbm.pred.m > 0.5)
```
Because the model never predicted any individual was male, the F1 Score of the GBM model cannot be defined. There is not a significant difference in heights and weights between males and females in this data set, so the GBM model attains the best results if it always predicts that every individual is female.  Since the majority of individuals in the training set were females, this actually is a good decision.  The accuracy of this model was not worse than the balanced models (`r fifty.accuracy`).


## 4 - ROC Curve

```{r, message = FALSE}
## Rock curve
little.roc <- validate.m %>% pull(Gender)
roc.create <- roc(little.roc, gbm.pred.m)

## Area Under the Curve
roc.area <- round(auc(roc.create), 3)
```


The following plot is the ROC Curve for the GBM with 50 Males:



```{r, echo = FALSE, message = FALSE}
plot(roc.create, legacy.axes = TRUE, main = "ROC Curve")
```




The AUC for this ROC Curve is `r roc.area`.  This means that the GBM model for the 50 males model only does a slightly better job at predicting gender based on height and weight than a model that randomly guessed (AUC = 0.50).  Seeing as there is no difference in height and weight distributions between males and females in this randomly generated data, the AUC value makes sense.

### 5 - K-Means
```{r}
# Prepare Table
kmean.df <- gen %>% select(c(Height, Weight))

# KMeans
cluster.det <- kmeans(kmean.df, 2)

# Assign Cluster nums
kmean.df$cluster.num <- as.character(cluster.det$cluster)
```

The graph below illustrates the two clustering groups created by the K-Means model.

```{r echo = FALSE}
# Graph
ggplot(kmean.df, aes(Height, Weight, color = cluster.num)) +
	geom_point() +
	labs(color = "Cluster") +
	scale_color_manual(labels = c("Group 1", "Group 2"), values = c("blue", "red")) +
	ggtitle("Predicted Gender from Height and Weight")
```

It is not possible to determine which group is male or female since the distributions of height and weight were extremely similar between the two groups.  It appears that the K-Means model split the individuals up into the groups based on whether individuals weighed more or less than 105 lbs.  There is an equal chance of either cluster representing the predicted male or female group. 